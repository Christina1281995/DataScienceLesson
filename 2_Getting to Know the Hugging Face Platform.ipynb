{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](https://github.com/Christina1281995/demo-repo/blob/main/header_noteook_2.png?raw=true)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An introduction to Hugging Face"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-\tHugging Face is a leading open-source library and platform for NLP tasks\n",
        "\n",
        "-\tThe library includes:\n",
        "\n",
        "    -\tpre-trained <b>models </b>\n",
        "    -\t<b>tools</b> for building custom NLP models\n",
        "    -\teasy-to-use <b>pipeline</b> approach\n",
        "    -\tsupport for a wide range of programming <b>languages</b> (Python, Java, JavaScript …)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look to see what it actually looks like:\n",
        "\n",
        "https://huggingface.co/ "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <img src=\"https://github.com/Christina1281995/demo-repo/blob/main/models.PNG?raw=true\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The \"Models\" page on Hugging Face is a <b>searchable database</b> of over 170,000 pre-trained models for natural language processing (NLP), computer vision, and speech recognition. <br><br>\n",
        "The page allows users to search for models by <b>task, framework, language, and model architecture</b>. \n",
        "<br><br>\n",
        "Additionally, the page provides a <b>leaderboard</b> of top-performing models for various tasks, as well as a section for community-contributed models. The models available on Hugging Face are designed to be easily integrated into existing projects and workflows, and the site provides tools and resources to help developers use them effectively.\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/modelcard1.PNG?raw=true\" width=\"80%\" align=\"right\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<b>Each model</b> has a profile page that includes:\n",
        "\n",
        "- a description\n",
        "\n",
        "- performance metrics\n",
        "\n",
        "- a list of available implementations in various frameworks\n",
        "\n",
        "- links to download the model \n",
        "\n",
        "- links to its source code on GitHub"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you take a look at the \"Files and version\" tab on a model you will see that they all have a few essential building blocks.\n",
        "\n",
        "For the model itself:\n",
        "- a `config.json` file, which saves the configuration of your model ;\n",
        "- a `pytorch_model.bin` file, which is the PyTorch checkpoint (unless you can’t have it for some reason) ;\n",
        "- a `tf_model.h5` file, which is the TensorFlow checkpoint (unless you can’t have it for some reason) ;\n",
        "\n",
        "For the tokenizer:\n",
        "- a `special_tokens_map.json`, which is part of your tokenizer save;\n",
        "- a `tokenizer_config.json`, which is part of your tokenizer save;\n",
        "- files named `vocab.json`, `vocab.txt`, `merges.txt`, or similar, which contain the vocabulary of your tokenizer, part of your tokenizer save;\n",
        "- maybe a `added_tokens.json`, which is part of your tokenizer save."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thanks to those standardised files, we can use a pipeline() method to easily use any of those models! "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yzddzVLO-jxO"
      },
      "source": [
        "##### Pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0IZdkZBD-jxP"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) is the easiest and fastest way to use a pretrained model for inference.\n",
        "\n",
        "There are <b>three main steps</b> involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is preprocessed into a format the model can understand.\n",
        "2. The preprocessed inputs are passed to the model.\n",
        "3. The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg\" width=\"60%\">\n",
        "\n",
        "\n",
        "By <b>default</b>, the pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
        "\n",
        "```python\n",
        "\n",
        "    from transformers import pipeline\n",
        "    classifier = pipeline(\"sentiment-analysis\")\n",
        "    classifier(\"Today was just terrible!!\")\n",
        "\n",
        "```\n",
        "\n",
        "| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n",
        "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n",
        "| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=“sentiment-analysis”)           |\n",
        "| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=“text-generation”)              |\n",
        "| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=“summarization”)                |\n",
        "| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=“image-classification”)         |\n",
        "| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=“image-segmentation”)           |\n",
        "| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=“object-detection”)             |\n",
        "| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=“audio-classification”)         |\n",
        "| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=“automatic-speech-recognition”) |\n",
        "| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=“vqa”)                          |\n",
        "| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n",
        "| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "raUEr7nV-jxQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9996246099472046}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"Today was just terrible!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/Christina1281995/demo-repo/blob/main/task.PNG?raw=true\" align=\"right\" width=\"30%\">\n",
        "If we don't just want to use the default model, we can also choose a particular model from the Hub. \n",
        "\n",
        "All we need to do is go to the [Model Hub](https://huggingface.co/models) and click on the corresponding tag on the left to display only the supported models for that task. You should get to a page like this one.\n",
        "\n",
        "```python\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "classifier(\"Today was just terrible!\")\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 747/747 [00:00<00:00, 373kB/s]\n",
            "c:\\Users\\Christina\\anaconda3\\envs\\lesson\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Christina\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading pytorch_model.bin: 100%|██████████| 499M/499M [00:10<00:00, 49.8MB/s] \n",
            "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 7.42MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.90MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 150/150 [00:00<00:00, 49.9kB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_0', 'score': 0.9787670373916626}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "classifier(\"Today was just terrible!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lesson_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
